{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:51:34.061293Z","iopub.execute_input":"2023-11-11T18:51:34.061911Z","iopub.status.idle":"2023-11-11T18:51:46.950516Z","shell.execute_reply.started":"2023-11-11T18:51:34.061878Z","shell.execute_reply":"2023-11-11T18:51:46.949461Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.17.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Dropout, GlobalMaxPooling1D, Bidirectional,Conv1D,MaxPooling1D,Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:51:46.952633Z","iopub.execute_input":"2023-11-11T18:51:46.953032Z","iopub.status.idle":"2023-11-11T18:52:03.010867Z","shell.execute_reply.started":"2023-11-11T18:51:46.952991Z","shell.execute_reply":"2023-11-11T18:52:03.010023Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:03.012124Z","iopub.execute_input":"2023-11-11T18:52:03.012831Z","iopub.status.idle":"2023-11-11T18:52:04.367383Z","shell.execute_reply.started":"2023-11-11T18:52:03.012792Z","shell.execute_reply":"2023-11-11T18:52:04.366479Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle=preprocessing.LabelEncoder()\ny=le.fit_transform(df['sentiment'])","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:04.369746Z","iopub.execute_input":"2023-11-11T18:52:04.370103Z","iopub.status.idle":"2023-11-11T18:52:04.399120Z","shell.execute_reply.started":"2023-11-11T18:52:04.370046Z","shell.execute_reply":"2023-11-11T18:52:04.398229Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype):\n/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n","output_type":"stream"}]},{"cell_type":"code","source":"#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndf['review'] = df['review'].apply(lambda z: remove_punctuations(z))\ndf['review'] = df['review'].apply(lambda z: remove_html(z))\ndf['review'] = df['review'].apply(lambda z: remove_url(z))\ndf['review'] = df['review'].apply(lambda z: remove_emoji(z))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:04.400439Z","iopub.execute_input":"2023-11-11T18:52:04.400796Z","iopub.status.idle":"2023-11-11T18:52:14.452667Z","shell.execute_reply.started":"2023-11-11T18:52:04.400765Z","shell.execute_reply":"2023-11-11T18:52:14.451874Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nimport tokenizers\nfrom transformers import RobertaTokenizer, TFRobertaModel\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:14.453890Z","iopub.execute_input":"2023-11-11T18:52:14.454271Z","iopub.status.idle":"2023-11-11T18:52:14.491004Z","shell.execute_reply.started":"2023-11-11T18:52:14.454239Z","shell.execute_reply":"2023-11-11T18:52:14.490259Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:14.492140Z","iopub.execute_input":"2023-11-11T18:52:14.492483Z","iopub.status.idle":"2023-11-11T18:52:14.496903Z","shell.execute_reply.started":"2023-11-11T18:52:14.492452Z","shell.execute_reply":"2023-11-11T18:52:14.495850Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = 'roberta-base'\nMAX_LEN = 256\nARTIFACTS_PATH = '../artifacts/'\n\nBATCH_SIZE = 16\nEPOCHS = 3\n\nif not os.path.exists(ARTIFACTS_PATH):\n    os.makedirs(ARTIFACTS_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:14.497910Z","iopub.execute_input":"2023-11-11T18:52:14.498194Z","iopub.status.idle":"2023-11-11T18:52:14.506813Z","shell.execute_reply.started":"2023-11-11T18:52:14.498170Z","shell.execute_reply":"2023-11-11T18:52:14.506067Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def roberta_encode(texts, tokenizer):\n    ct = len(texts)\n    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n\n    for k, text in enumerate(texts):\n        # Tokenize\n        tok_text = tokenizer.tokenize(text)\n        \n        # Truncate and convert tokens to numerical IDs\n        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n        \n        input_length = len(enc_text) + 2\n        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n        \n        # Add tokens [CLS] and [SEP] at the beginning and the end\n        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n        \n        # Set to 1s in the attention input\n        attention_mask[k,:input_length] = 1\n\n    return {\n        'input_word_ids': input_ids,\n        'input_mask': attention_mask,\n        'input_type_ids': token_type_ids\n    }","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:14.508345Z","iopub.execute_input":"2023-11-11T18:52:14.508675Z","iopub.status.idle":"2023-11-11T18:52:14.518347Z","shell.execute_reply.started":"2023-11-11T18:52:14.508644Z","shell.execute_reply":"2023-11-11T18:52:14.517477Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_data = df['review'].to_numpy().reshape(-1)\ny_data = y","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:14.521418Z","iopub.execute_input":"2023-11-11T18:52:14.521765Z","iopub.status.idle":"2023-11-11T18:52:14.529911Z","shell.execute_reply.started":"2023-11-11T18:52:14.521740Z","shell.execute_reply":"2023-11-11T18:52:14.529140Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train1, X_test, y_train1, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=777) # random_state to reproduce results\n\n\nX_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1 , test_size=0.2, random_state=777)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:14.530962Z","iopub.execute_input":"2023-11-11T18:52:14.531376Z","iopub.status.idle":"2023-11-11T18:52:14.550955Z","shell.execute_reply.started":"2023-11-11T18:52:14.531351Z","shell.execute_reply":"2023-11-11T18:52:14.549903Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Import tokenizer from HuggingFace\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:14.552324Z","iopub.execute_input":"2023-11-11T18:52:14.552629Z","iopub.status.idle":"2023-11-11T18:52:16.916856Z","shell.execute_reply.started":"2023-11-11T18:52:14.552603Z","shell.execute_reply":"2023-11-11T18:52:16.915766Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742b8527ad474a8a8a9748532bac8c74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb82847664b4b378e785bdb442c7f55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b7821efed7148a59771625940a65c9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290fd3c8416944b09bc1f8709f16fe85"}},"metadata":{}}]},{"cell_type":"code","source":"X_train = roberta_encode(X_train, tokenizer)\nX_test = roberta_encode(X_test, tokenizer)\nX_val = roberta_encode(X_val, tokenizer)\ny_train = np.asarray(y_train, dtype='int32')\ny_test = np.asarray(y_test, dtype='int32')\ny_val = np.asarray(y_val, dtype='int32')","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:52:16.918302Z","iopub.execute_input":"2023-11-11T18:52:16.918668Z","iopub.status.idle":"2023-11-11T18:54:16.259978Z","shell.execute_reply.started":"2023-11-11T18:52:16.918639Z","shell.execute_reply":"2023-11-11T18:54:16.259119Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def build_model(n_categories):\n        input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n        input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n        input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n\n        # Import RoBERTa model from HuggingFace\n        roberta_model = TFRobertaModel.from_pretrained(MODEL_NAME)\n        x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n\n        # Huggingface transformers have multiple outputs, embeddings are the first one,\n        # so let's slice out the first position\n        x = x[0]\n\n        x = tf.keras.layers.Dropout(0.1)(x)\n        x = tf.keras.layers.Flatten()(x)\n        x = tf.keras.layers.Dense(256, activation='relu')(x)\n        x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n        \n        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-5), metrics=['accuracy'])\n        return model","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:54:16.261131Z","iopub.execute_input":"2023-11-11T18:54:16.261424Z","iopub.status.idle":"2023-11-11T18:54:16.270235Z","shell.execute_reply.started":"2023-11-11T18:54:16.261399Z","shell.execute_reply":"2023-11-11T18:54:16.269110Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = build_model(3)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:54:16.271435Z","iopub.execute_input":"2023-11-11T18:54:16.271725Z","iopub.status.idle":"2023-11-11T18:54:33.924020Z","shell.execute_reply.started":"2023-11-11T18:54:16.271700Z","shell.execute_reply":"2023-11-11T18:54:33.923125Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20aa598b70be4a83b4cd63dfee3bdcf2"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_word_ids (InputLayer  [(None, 256)]                0         []                            \n )                                                                                                \n                                                                                                  \n input_mask (InputLayer)     [(None, 256)]                0         []                            \n                                                                                                  \n input_type_ids (InputLayer  [(None, 256)]                0         []                            \n )                                                                                                \n                                                                                                  \n tf_roberta_model (TFRobert  TFBaseModelOutputWithPooli   1246456   ['input_word_ids[0][0]',      \n aModel)                     ngAndCrossAttentions(last_   32         'input_mask[0][0]',          \n                             hidden_state=(None, 256, 7              'input_type_ids[0][0]']      \n                             68),                                                                 \n                              pooler_output=(None, 768)                                           \n                             , past_key_values=None, hi                                           \n                             dden_states=None, attentio                                           \n                             ns=None, cross_attentions=                                           \n                             None)                                                                \n                                                                                                  \n dropout_37 (Dropout)        (None, 256, 768)             0         ['tf_roberta_model[0][0]']    \n                                                                                                  \n flatten (Flatten)           (None, 196608)               0         ['dropout_37[0][0]']          \n                                                                                                  \n dense (Dense)               (None, 256)                  5033190   ['flatten[0][0]']             \n                                                          4                                       \n                                                                                                  \n dense_1 (Dense)             (None, 1)                    257       ['dense[0][0]']               \n                                                                                                  \n==================================================================================================\nTotal params: 174977793 (667.49 MB)\nTrainable params: 174977793 (667.49 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#create callback\nfilepath = 'my_best_model.hdf5'\ncheckpoint = ModelCheckpoint(filepath=filepath, \n                             monitor='val_loss',\n                             verbose=1, \n                             save_best_only=True,\n                             mode='min')\ncallbacks = [checkpoint]","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:54:33.925435Z","iopub.execute_input":"2023-11-11T18:54:33.925805Z","iopub.status.idle":"2023-11-11T18:54:33.931552Z","shell.execute_reply.started":"2023-11-11T18:54:33.925771Z","shell.execute_reply":"2023-11-11T18:54:33.930446Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train,\n                        y_train,\n                        epochs=EPOCHS,\n                        batch_size=BATCH_SIZE,\n                        verbose=1,\n                        validation_data=(X_val,y_val))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:54:33.932856Z","iopub.execute_input":"2023-11-11T18:54:33.933229Z","iopub.status.idle":"2023-11-11T19:48:21.006118Z","shell.execute_reply.started":"2023-11-11T18:54:33.933196Z","shell.execute_reply":"2023-11-11T19:48:21.005108Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/3\n2000/2000 [==============================] - 1129s 545ms/step - loss: 0.2114 - accuracy: 0.9167 - val_loss: 0.1601 - val_accuracy: 0.9396\nEpoch 2/3\n2000/2000 [==============================] - 1050s 525ms/step - loss: 0.1298 - accuracy: 0.9532 - val_loss: 0.1839 - val_accuracy: 0.9319\nEpoch 3/3\n2000/2000 [==============================] - 1045s 523ms/step - loss: 0.0625 - accuracy: 0.9784 - val_loss: 0.2379 - val_accuracy: 0.9365\n","output_type":"stream"}]},{"cell_type":"code","source":"\ny_pred = [np.argmax(i) for i in model.predict(X_test)]","metadata":{"execution":{"iopub.status.busy":"2023-11-11T19:48:21.009817Z","iopub.execute_input":"2023-11-11T19:48:21.010162Z","iopub.status.idle":"2023-11-11T19:50:04.098023Z","shell.execute_reply.started":"2023-11-11T19:48:21.010134Z","shell.execute_reply":"2023-11-11T19:50:04.097140Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"313/313 [==============================] - 103s 319ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2023-11-11T19:50:04.099341Z","iopub.execute_input":"2023-11-11T19:50:04.099662Z","iopub.status.idle":"2023-11-11T19:51:47.662106Z","shell.execute_reply.started":"2023-11-11T19:50:04.099635Z","shell.execute_reply":"2023-11-11T19:51:47.661033Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Accuracy: 94.32%\n","output_type":"stream"}]}]}